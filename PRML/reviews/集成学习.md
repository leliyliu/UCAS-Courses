# 集成学习

[toc]

集成学习的思路就是用很少量的工作，组合多个基模型，提供系统性能。 

## NFL 定理（No Free Lunch Theorem）

没有任何学习算法可以在任何领域总是产生最准确的学习器。 实际上，NFL 定理中有个重要前提，即所有问题出现的机会相同，或者说所有问题同等重要。 然而实际中，一个特定的模型或者一个特定的问题，必然有所不同适应。因此具体问题具体分析，使用不同的机器学习方法（模型），针对性解决。 

### 丑小鸭定理

分类结果取决于选择什么特征作为分类标准，而特征的选择又依存于人的目的

### 奥卡姆剃刀原理

在各种候选的假设中，应选择假设最少的假设（只需要考虑必要的参考内容） 。本质上，奥卡姆剃刀原理的关注点是模型复杂度，从而能够避免过拟合或者欠拟合。 

#### 偏差和方差（欠拟合与过拟合）

在模型当中，偏差和方差是两个不同的概念，一般而言，模型的总体误差可以表示为如下： 

![image-20211214134705935](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214134705935.png)

其中，$\hat{f}(x)$ 表示的模型预测值，而$f(x)$是真实模型，有一定的误差。 其具体推导过程为： （下图有误，中间添加的部分应该是$\overline{f}(x)$而不是$f(x)$。 

![image-20211214135224956](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214135224956.png)

本质而言，偏差就是模型预测值与真实规律之间的差异： $bias(\hat{f}) = E[\hat{f}(x)] - f(x)$，而偏差来源于模型中的错误假设，偏差过高就意味着模型所代表的特征和标签之间的关系是错误的，对应欠拟合现象。

方差应该被表示为: $Var(\hat{f}) = E[(\hat{f}(x) - E[\hat{f}(x)])^2]$，其描述了通过学习你和出来的结果自身的不稳定性。 方差来源于模型对训练数据波动的过度敏感，方差过高意味着模型对数据中的随机噪声也进行了建模，将本不属于“特征” - “标签” 关系中的随机特征也纳入到模型之中，对应着过拟合现象。 

#### 偏差方差平衡

一般而言有这样的规律： 简单的模型偏差高，方差低；而复杂的模型偏差低，方差高。 因此要选择一个合适复杂度的模型：总体而言，可以总结为如下

![image-20211214141219640](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214141219640.png)

#### 模型校验

##### 使用校验集

也就是留出校验集来对于模型的效果进行判定

##### K-折校验

在数据量较少的时候，很难留出足够的数据进行校验。那么就利用K-折交叉验证的方式来进行校验。 

##### 网格搜索

![image-20211214141752022](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214141752022.png)

## Resampling (重采样)

一般而言，有两种不同的重采样方法，Jackknife （无放回）和 Bootstrap （有放回）。 这里只介绍Bootstrap方法。

### Bootstrap

其具体而言，是从原始的N个样本数据 $\mathcal{D} = \{x_1, ..., x_N\}$进行N次（次数是一致的）有放回采样，得到新的数据$\mathcal{D'}$，实际上等价于给样本进行了reweighting。 

## Bagging

bagging 的思路是比较简单的，也就是对给定的N个样本的数据集$\mathcal{D}$ 进行bootstrap 采样，得到新的数据集$\mathcal{D^1}$，并在其上训练模型$f^1$。重复上述过程$M$次，得到$M$个模型，则最终的结果为： 

![image-20211214142951199](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214142951199.png)

这种方法可以降低模型的方差（避免过拟合），这里不再给出推导。 值给出一个说明： 若$f_m$之间的相关性为$\rho$，则$f_{avg}$的方差为： $\rho \times \sigma^2 + (1-\rho) \times \frac{\sigma^2}{M}$

故实际上，Bagging 方法适合对偏差低、方差高的模型进行融合（决策树，神经网络） 

#### Out-of-bag Error （OOBE）

在bagging 中，每个基学习器只使用了原始数据集中的一部分进行训练，所以可以不用交叉验证，直接用包外样本的误差来估计它的泛化误差/测试误差。

#### 基学习器数目

![image-20211214143457382](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214143457382.png)

#### 随机森林

对于决策树算法，如果直接使用Bagging 方法，得到的多棵树高度相关，带来的方差减少有限，因此随机森林通过随机选择一部分特征和一部分样本来降低树的相关性。 

## Boosting

Boosting 的基本思想就是： 将弱学习器组合成强分类器。 其学习框架是顺序学习多个弱学习器（顺序学习！！） 

一个例子是： 

![image-20211214153649320](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214153649320.png)

可以看到，这里不同的弱分类器利用的是不同的数据，这里和bagging 是不同的。 

互补的学习器： 可以在不同的训练集上训练学习器，这些不同的训练集可以通过对于原始训练集进行重采样或者对原始训练集重新加权。 

### AdaBoost

在弱学习器$\phi_1$失败的样本上学习第二个弱学习器$\phi_2$， 也就是： 

![image-20211214154039290](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214154039290.png)

在这一过程中，需要对样本进行重加权：对于分对的样本，其权重减少，而对于分错的样本，其权重增大。 

![image-20211214154339110](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214154339110.png)

#### 优点和缺点

优点包括了实现快速、简单；灵活（弱学习器的构建可以用简单算法， 需要使用的先验知识或者假设限制比较少）；并且通用性高，可以使用不同模态的数据和多类别数据。 

缺点是AdaBoost 的性能取决于数据和弱学习器，如果弱分类器过于复杂，可能会过拟合，或者弱分类器太弱，以及有可能受到均匀噪声的影响。 

## Stacking

Stacking 是一个分层的结构，具体架构如下： 

![image-20211214154949896](https://gitee.com/leliyliu/blog-image/raw/master/img%20/image-20211214154949896.png)